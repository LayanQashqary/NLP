# -*- coding: utf-8 -*-
"""NLP-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18gX5n_uHkmz6iKV6GQLn13XvkFN-n6k5
"""

# - - - START - - -
#Importing necessary libraries that we will need to use later
import numpy as np
import pandas as pd
import seaborn as sns
import nltk as nltk

"""# **Load Data**"""

#Loading (Arabic) dataset from CSV file into a Pandas DataFrame
Data_set = pd.read_csv("/content/NewDA.csv", sep=';' , header=None)

#Renaming columns
Data_set.columns = ['Sentence', 'Personality']

#Displaying information about the dataset
Data_set.info()

#Displaying descriptive statistics of the dataset
Data_set.describe()

#Displaying first 10 rows of the dataset
Data_set.head(10)

"""# **Preprocessing/Cleaning Data**

# **1.  Encode the Values in the "Personality" Column into Numeric Representations**
"""

from sklearn.preprocessing import LabelEncoder

def personalType():
    # Extract the "Personality" column from the Data_set
    text_column = Data_set["Personality"]

    # Fill any missing values in the "Personality" column with an empty string
    text_column.fillna('', inplace=True)

    # Encode the text column using label encoding
    label_encoder = LabelEncoder()
    numeric_column = label_encoder.fit_transform(text_column)

    # Return the encoded numeric column
    return numeric_column

# Apply the personalType function to the "Personality" column in Data_set
Data_set["Personality"] = personalType()
# Display the first 3 rows of the updated Data_set
Data_set.head(10)

"""# **2. Remove punctuation**"""

#A variable that contains all Arabic punctuation characters
arabic_punctuation = "؛،؟٬٫٪ـ.،/()[]{}:;\"'‘’“”…«»-–—_!"

def remove_punct(text):

   #Joining characters that are not in (string.punctuation) list
   text_nonpunct = "".join([char for char in text if char not in arabic_punctuation])
   return text_nonpunct

#Creating new columns (Arabic_nonpunct)
Data_set['Nonpunct'] = Data_set['Sentence'].apply(lambda x: remove_punct(x))
Data_set.head(3)

"""# **3. Tokenization**"""

#Importing functions from (NLTK) for text tokenization
from nltk.tokenize import word_tokenize

#Tokenizer data
nltk.download('punkt')

def tokenize(text):
  #Tokenizing the input text
  tokens = word_tokenize(text)
  return tokens

#Creating new columns (Tokenized)
Data_set['Tokenized'] = Data_set['Nonpunct'].apply(lambda x: tokenize(x))
Data_set.head(3)

"""# **4. Remove Stopwords Except  (أنا, أنتم , أنت )**






"""

#Importing functions from (NLTK) for removing stopwords
from nltk.corpus import stopwords

#Stopwords data
nltk.download('stopwords')

#Geting Arabic stopwords
arabic_stopwords = set(stopwords.words('arabic'))
arabic_stopwords.remove('أنا')
arabic_stopwords.remove('أنت')
arabic_stopwords.remove('أنتم')

def remove_stopwords(tokenized_list):
  text = [word for word in tokenized_list if word not in arabic_stopwords]
  return text

#Creating new column (Nonstop)
Data_set['Nonstop'] = Data_set['Tokenized'].apply(lambda x: remove_stopwords(x))
Data_set.head(3)

"""

```
# This is formatted as code
```

# **5. Lemmatization**"""

!pip install qalsadi
from qalsadi.lemmatizer import Lemmatizer
lemmatizer = Lemmatizer()

def lemmatization(text):
    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]
    return lemmatized_text

Data_set['Lemmatized'] = Data_set['Nonstop'].apply(lambda x: lemmatization(x))

#show table
Data_set.head(3)

"""# **6. Convert a List of Tokens into a Clean Text**"""

def to_text(text):
    # Joining the list of tokens into a single string separated by spaces
    token_to_text = ' '.join(text)
    return token_to_text

# Apply the to_text function to the 'Lemmatized' column and create a new column 'clean_text'
Data_set['clean_text'] = Data_set['Lemmatized'].apply(to_text)

# Drop unnecessary columns from the Data_set
Data_set = Data_set.drop(columns=['Nonpunct', 'Tokenized', 'Nonstop', 'Lemmatized'])

# Display the first 3 rows of the updated Data_set
Data_set.head(3)

"""# **Feature Engineering**

#**1. Count most used words in leader and follower's statements.**
"""

from collections import Counter

# List of sentences
sentences = Data_set['clean_text']

# Flatten the list of sentences into a single list of words
all_words = [word for sentence in sentences for word in sentence.split()]

# Create a Counter object to count word frequencies across all sentences
word_counts = Counter(all_words)

# Print the most common words and their frequencies for the Follower personality
print('Most Frequent Words in Follower Personality')
Most_Freq_Follower = []

for word, count in word_counts.most_common():
    if Data_set['Personality'].iloc[count] == 0 and count > 40:  # Combined conditions
        Most_Freq_Follower.append(word)  # Append word to the list
        print(f"{word}, Frequency: {count}")

# Print the most common words and their frequencies for the Leadership personality
print('Most Frequent Words in Leadership Personality')
Most_Freq_Leader = []

for word, count in word_counts.most_common():
    if Data_set['Personality'].iloc[count] == 1 and count > 40:  # Combined conditions
        Most_Freq_Leader.append(word)  # Append word to the list
        print(f"{word}, Frequency: {count}")

Data_set['Most_freq_Leader_Words'] = 0
Data_set['Most_freq_Follower_Words'] = 0

# Count the occurrences of most frequent words in the 'clean_text' column for the Leadership personality
for leader in Most_Freq_Leader:
    Data_set['Most_freq_Leader_Words'] += Data_set['clean_text'].apply(lambda x: x.count(leader))

# Count the occurrences of most frequent words in the 'clean_text' column for the Follower personality
for follower in Most_Freq_Follower:
    Data_set['Most_freq_Follower_Words'] += Data_set['clean_text'].apply(lambda x: x.count(follower))

# Display the first 3 rows of the updated Data_set
Data_set.head(3)

"""# **2. TF-IDF Vectorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

tfidf = TfidfVectorizer()  # Create a TfidfVectorizer object

# Fit the TfidfVectorizer on the 'clean_text' column and transform the data into TF-IDF features
td_features = tfidf.fit_transform(Data_set['clean_text'])

Td_features = pd.DataFrame(td_features.toarray())  # Convert the TF-IDF features to a DataFrame
Td_features.columns = tfidf.get_feature_names_out()  # Set column names using the feature names from TfidfVectorizer

Td_features  # Display the DataFrame of TF-IDF features

"""# **Machine Learning Model**

# **Marge All Features**
"""

labels = Data_set['Personality']
features = pd.concat([Td_features,Data_set['Most_freq_Follower_Words'],Data_set['Most_freq_Leader_Words']], axis=1)
features.shape

"""#**Split Data to (Train and Test)**"""

from sklearn.model_selection import train_test_split

# Split the features and labels into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, shuffle=True)

# Print the shape of the training and testing sets
print("Training set shapes:", x_train.shape, y_train.shape)
print("Testing set shapes:", x_test.shape, y_test.shape)

"""#**Random Forest Model**"""

from sklearn.metrics import classification_report

from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest Classifier model with specified parameters
rfc_model = RandomForestClassifier(n_estimators=50, n_jobs=-1)

# Fit the model on the training data
rfc_model.fit(x_train, y_train)

# Make predictions on the testing data
rfc_predictions = rfc_model.predict(x_test)

# Generate a classification report based on the predictions
rfc_report = classification_report(y_test, rfc_predictions)

# Print the classification report
print("Random Forest Classifier Report:")
print(rfc_report)

# Calculate and print the accuracy score of the model on the testing data
accuracy = rfc_model.score(x_test, y_test) * 100
print("Accuracy:", accuracy, "%")

"""#**SVM Model**"""

from sklearn import svm

# Create a Support Vector Machine (SVM) Classifier model with specified parameters
SVM_model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')

# Fit the model on the training data
SVM_model.fit(x_train, y_train)

# Make predictions on the testing data
SVM_predictions = SVM_model.predict(x_test)

# Generate a classification report based on the predictions
SVM_report = classification_report(y_test, SVM_predictions)

# Print the classification report
print("SVM Classifier Report:")
print(SVM_report)

# Calculate and print the accuracy score of the model on the testing data
accuracy = SVM_model.score(x_test, y_test) * 100
print("Accuracy:", accuracy, "%")